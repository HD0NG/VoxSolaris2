{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "\n",
    "import laspy\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "try:\n",
    "    from hdbscan import HDBSCAN\n",
    "    HAS_HDBSCAN = True\n",
    "except ImportError:\n",
    "    HAS_HDBSCAN = False\n",
    "\n",
    "import open3d as o3d\n",
    "\n",
    "try:\n",
    "    from shapely.geometry import shape\n",
    "    from shapely.vectorized import contains\n",
    "    import fiona\n",
    "    HAS_SHAPELY = True\n",
    "except ImportError:\n",
    "    HAS_SHAPELY = False\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# CONFIGURATION\n",
    "# =====================================================================\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    \"\"\"Central configuration for the full pipeline.\"\"\"\n",
    "    # Crop\n",
    "    bounds: Optional[Dict[str, float]] = None\n",
    "    clip_polygon_path: Optional[str] = None\n",
    "\n",
    "    # SOR\n",
    "    sor_nb_neighbors: int = 20\n",
    "    sor_std_ratio: float = 3.0\n",
    "    sor_chunk_size: int = 5_000_000\n",
    "\n",
    "    # Class filter\n",
    "    valid_classes: List[int] = field(default_factory=lambda: [2, 3, 4, 5, 6])\n",
    "\n",
    "    # HAG\n",
    "    hag_k_neighbors: int = 4\n",
    "\n",
    "    # --- Building reclassification (NEW) ---\n",
    "    reclass_scales: List[int] = field(default_factory=lambda: [10, 20, 40])\n",
    "    reclass_min_hag: float = 2.0            # metres — ignore anything below this\n",
    "    reclass_max_hag: float = 50.0           # metres — ignore anything above this\n",
    "    reclass_score_threshold: float = 0.55   # combined score to accept as building\n",
    "    reclass_ransac_distance: float = 0.15   # RANSAC plane inlier distance (m)\n",
    "    reclass_ransac_iterations: int = 100\n",
    "    reclass_normal_std_threshold: float = 0.25  # max normal-Z std for building\n",
    "\n",
    "    # Region growing\n",
    "    rg_search_radius: float = 1.5     # metres\n",
    "    rg_z_tolerance: float = 2.0       # metres\n",
    "    rg_min_score: float = 0.40        # relaxed threshold for region-grown points\n",
    "    rg_normal_agreement: float = 0.85 # cosine similarity of normals for growing\n",
    "\n",
    "    # Cluster cleanup\n",
    "    cluster_eps: float = 1.5\n",
    "    cluster_min_size: int = 100\n",
    "    use_hdbscan: bool = True\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# MERGE (unchanged from previous upgrade)\n",
    "# =====================================================================\n",
    "def merge_las_files(paths: List[str], output_path: str) -> None:\n",
    "    if len(paths) < 2:\n",
    "        raise ValueError(\"Provide at least two files to merge.\")\n",
    "\n",
    "    log.info(\"Reading %d files for merge...\", len(paths))\n",
    "    datasets = [laspy.read(p) for p in paths]\n",
    "\n",
    "    fmt_id = datasets[0].header.point_format.id\n",
    "    for i, ds in enumerate(datasets[1:], start=1):\n",
    "        if ds.header.point_format.id != fmt_id:\n",
    "            raise ValueError(f\"Point format mismatch: file 0={fmt_id}, file {i}={ds.header.point_format.id}\")\n",
    "\n",
    "    ref = datasets[0]\n",
    "    new_header = laspy.LasHeader(point_format=ref.header.point_format, version=ref.header.version)\n",
    "    new_header.offsets = ref.header.offsets\n",
    "    new_header.scales = ref.header.scales\n",
    "    new_header.vlrs = ref.header.vlrs\n",
    "\n",
    "    merged_array = np.concatenate([ds.points.array for ds in datasets])\n",
    "    merged_las = laspy.LasData(new_header)\n",
    "    merged_las.points = laspy.ScaleAwarePointRecord(\n",
    "        merged_array, ref.header.point_format, ref.header.scales, ref.header.offsets\n",
    "    )\n",
    "    merged_las.header.mins = np.array([merged_las.x.min(), merged_las.y.min(), merged_las.z.min()])\n",
    "    merged_las.header.maxs = np.array([merged_las.x.max(), merged_las.y.max(), merged_las.z.max()])\n",
    "\n",
    "    log.info(\"Writing %s merged points to %s\", f\"{len(merged_array):,}\", output_path)\n",
    "    merged_las.write(output_path)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# CROP (unchanged)\n",
    "# =====================================================================\n",
    "def crop_las_file(\n",
    "    input_path: str, output_path: str,\n",
    "    bounds: Optional[Dict[str, float]] = None,\n",
    "    polygon_path: Optional[str] = None,\n",
    ") -> None:\n",
    "    las = laspy.read(input_path)\n",
    "    log.info(\"Loaded %s points from %s\", f\"{len(las.points):,}\", input_path)\n",
    "\n",
    "    if polygon_path and HAS_SHAPELY:\n",
    "        with fiona.open(polygon_path) as src:\n",
    "            geom = shape(src[0][\"geometry\"])\n",
    "        mask = contains(geom, las.x, las.y)\n",
    "    elif bounds:\n",
    "        mask = (\n",
    "            (las.x >= bounds['min_x']) & (las.x <= bounds['max_x']) &\n",
    "            (las.y >= bounds['min_y']) & (las.y <= bounds['max_y'])\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Provide either `bounds` dict or `polygon_path`.\")\n",
    "\n",
    "    cropped = las.points[mask]\n",
    "    if len(cropped) == 0:\n",
    "        log.error(\"No points within the specified region!\")\n",
    "        return\n",
    "\n",
    "    new_header = laspy.LasHeader(point_format=las.header.point_format, version=las.header.version)\n",
    "    new_header.offsets = las.header.offsets\n",
    "    new_header.scales = las.header.scales\n",
    "    new_header.vlrs = las.header.vlrs\n",
    "    out = laspy.LasData(new_header)\n",
    "    out.points = cropped\n",
    "    log.info(\"Saving %s cropped points.\", f\"{len(cropped):,}\")\n",
    "    out.write(output_path)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# SOR + CLASS FILTER (unchanged)\n",
    "# =====================================================================\n",
    "def filter_lidar_data(\n",
    "    input_path: str, output_path: str,\n",
    "    nb_neighbors: int = 20, std_ratio: float = 2.0,\n",
    "    valid_classes: Optional[List[int]] = None,\n",
    "    chunk_size: int = 5_000_000,\n",
    ") -> None:\n",
    "    if valid_classes is None:\n",
    "        valid_classes = [2, 3, 4, 5, 6]\n",
    "\n",
    "    las = laspy.read(input_path)\n",
    "    n_orig = len(las.points)\n",
    "    points = np.vstack((las.x, las.y, las.z)).T\n",
    "\n",
    "    log.info(\"Running chunked SOR on %s points (k=%d, σ=%.1f)...\", f\"{n_orig:,}\", nb_neighbors, std_ratio)\n",
    "\n",
    "    if len(points) <= chunk_size:\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(points)\n",
    "        _, inlier_idx = pcd.remove_statistical_outlier(nb_neighbors=nb_neighbors, std_ratio=std_ratio)\n",
    "        keep = np.zeros(len(points), dtype=bool)\n",
    "        keep[inlier_idx] = True\n",
    "    else:\n",
    "        keep = np.ones(len(points), dtype=bool)\n",
    "        x_sorted = np.argsort(points[:, 0])\n",
    "        for start in range(0, len(points), chunk_size):\n",
    "            end = min(start + chunk_size, len(points))\n",
    "            cidx = x_sorted[start:end]\n",
    "            pcd = o3d.geometry.PointCloud()\n",
    "            pcd.points = o3d.utility.Vector3dVector(points[cidx])\n",
    "            _, inlier_local = pcd.remove_statistical_outlier(nb_neighbors=nb_neighbors, std_ratio=std_ratio)\n",
    "            outlier_local = np.setdiff1d(np.arange(len(cidx)), inlier_local)\n",
    "            keep[cidx[outlier_local]] = False\n",
    "\n",
    "    las = las[keep]\n",
    "    log.info(\"SOR removed %s outliers.\", f\"{n_orig - len(las.points):,}\")\n",
    "\n",
    "    las = las[np.isin(las.classification, valid_classes)]\n",
    "    log.info(\"After class filter: %s points.\", f\"{len(las.points):,}\")\n",
    "    las.write(output_path)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# HAG COMPUTATION (in-memory only — does NOT overwrite Z)\n",
    "# =====================================================================\n",
    "def _compute_hag(las, k: int = 4) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute Height Above Ground for every point using IDW interpolation\n",
    "    of the nearest ground (Class 2) points.\n",
    "\n",
    "    Returns an (N,) array of HAG values in metres.  The LAS object's Z\n",
    "    coordinates are **not modified** — original elevation is preserved.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    las : laspy.LasData  — already-loaded point cloud (read once, reused).\n",
    "    k   : int            — number of ground neighbours for IDW (default 4).\n",
    "    \"\"\"\n",
    "    ground_mask = las.classification == 2\n",
    "    if np.sum(ground_mask) == 0:\n",
    "        log.warning(\"No Class 2 ground. Estimating from lowest 1%%...\")\n",
    "        sorted_idx = np.argsort(las.z)\n",
    "        n_gnd = max(int(len(las.points) * 0.01), 10)\n",
    "        ground_mask = np.zeros(len(las.points), dtype=bool)\n",
    "        ground_mask[sorted_idx[:n_gnd]] = True\n",
    "\n",
    "    gnd_xy = np.vstack((las.x[ground_mask], las.y[ground_mask])).T\n",
    "    gnd_z = np.asarray(las.z[ground_mask], dtype=np.float64)\n",
    "    log.info(\"Computing HAG from %s ground points (IDW k=%d)...\", f\"{len(gnd_z):,}\", k)\n",
    "\n",
    "    tree = cKDTree(gnd_xy)\n",
    "    all_xy = np.vstack((las.x, las.y)).T\n",
    "    dists, indices = tree.query(all_xy, k=k)\n",
    "\n",
    "    if k == 1:\n",
    "        z_ref = gnd_z[indices]\n",
    "    else:\n",
    "        dists = np.maximum(dists, 1e-10)\n",
    "        w = 1.0 / dists\n",
    "        w /= w.sum(axis=1, keepdims=True)\n",
    "        z_ref = np.sum(w * gnd_z[indices], axis=1)\n",
    "\n",
    "    hag = np.maximum(np.asarray(las.z, dtype=np.float64) - z_ref, 0.0)\n",
    "    log.info(\"HAG range: %.2f – %.2f m\", np.min(hag), np.max(hag))\n",
    "    return hag\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# GEOMETRIC FEATURE ENGINE (NEW)\n",
    "# =====================================================================\n",
    "def _compute_eigenfeatures_batch(\n",
    "    coords: np.ndarray,\n",
    "    neighbor_indices: np.ndarray,\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute a full set of eigenvalue-derived geometric features for every point,\n",
    "    fully vectorised (no Python loop).\n",
    "\n",
    "    Returns dict with keys: planarity, sphericity, linearity, surface_variation,\n",
    "    omnivariance, anisotropy, eigenentropy, normal_z, normal_z_std.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    coords : (N, 3) candidate XYZ\n",
    "    neighbor_indices : (N, k) indices into coords\n",
    "    \"\"\"\n",
    "    N, k = neighbor_indices.shape\n",
    "    neighbors = coords[neighbor_indices]                    # (N, k, 3)\n",
    "\n",
    "    # --- Covariance ---\n",
    "    means = neighbors.mean(axis=1, keepdims=True)           # (N, 1, 3)\n",
    "    centered = neighbors - means                             # (N, k, 3)\n",
    "    covs = np.einsum('nki,nkj->nij', centered, centered) / (k - 1)  # (N, 3, 3)\n",
    "\n",
    "    # --- Eigendecomposition (ascending order) ---\n",
    "    eigvals, eigvecs = np.linalg.eigh(covs)                 # (N,3), (N,3,3)\n",
    "\n",
    "    # Clamp tiny negatives from numerical noise\n",
    "    eigvals = np.maximum(eigvals, 0.0)\n",
    "    e3, e2, e1 = eigvals[:, 0], eigvals[:, 1], eigvals[:, 2]\n",
    "    esum = e1 + e2 + e3\n",
    "\n",
    "    # Guard division\n",
    "    safe_e1 = np.where(e1 > 0, e1, 1.0)\n",
    "    safe_esum = np.where(esum > 0, esum, 1.0)\n",
    "\n",
    "    # --- Eigenvalue features ---\n",
    "    planarity = (e2 - e3) / safe_e1\n",
    "    linearity = (e1 - e2) / safe_e1\n",
    "    sphericity = e3 / safe_e1\n",
    "    anisotropy = (e1 - e3) / safe_e1\n",
    "    surface_variation = e3 / safe_esum\n",
    "    omnivariance = np.cbrt(e1 * e2 * e3)\n",
    "\n",
    "    # Eigenentropy — using normalised eigenvalues\n",
    "    en = eigvals / safe_esum[:, None]\n",
    "    en_safe = np.where(en > 0, en, 1.0)\n",
    "    eigenentropy = -np.sum(en * np.log(en_safe), axis=1)\n",
    "\n",
    "    # --- Normal vector features ---\n",
    "    # The normal is the eigenvector corresponding to the smallest eigenvalue (column 0)\n",
    "    normals = eigvecs[:, :, 0]  # (N, 3)\n",
    "    # Make normals consistently upward-pointing\n",
    "    flip = normals[:, 2] < 0\n",
    "    normals[flip] *= -1\n",
    "    normal_z = normals[:, 2]  # verticality: 1.0 = horizontal surface, 0.0 = vertical wall\n",
    "\n",
    "    # Normal-Z standard deviation within neighbourhood: measures normal consistency\n",
    "    # For each point, gather the normals of its k neighbours and compute std of their Z component\n",
    "    neighbour_nz = normal_z[neighbor_indices]               # (N, k)\n",
    "    normal_z_std = np.std(neighbour_nz, axis=1)             # (N,)\n",
    "\n",
    "    return {\n",
    "        'planarity': planarity,\n",
    "        'linearity': linearity,\n",
    "        'sphericity': sphericity,\n",
    "        'anisotropy': anisotropy,\n",
    "        'surface_variation': surface_variation,\n",
    "        'omnivariance': omnivariance,\n",
    "        'eigenentropy': eigenentropy,\n",
    "        'normal_z': normal_z,\n",
    "        'normal_z_std': normal_z_std,\n",
    "    }\n",
    "\n",
    "\n",
    "def _ransac_plane_inlier_ratio_batch(\n",
    "    coords: np.ndarray,\n",
    "    neighbor_indices: np.ndarray,\n",
    "    distance_threshold: float = 0.15,\n",
    "    n_iterations: int = 100,\n",
    "    rng_seed: int = 42,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    For each point's neighbourhood, estimate the best-fit plane via RANSAC\n",
    "    and return the fraction of neighbours that are inliers.\n",
    "\n",
    "    Buildings → high inlier ratio (0.8–1.0)\n",
    "    Vegetation → low inlier ratio (0.2–0.5)\n",
    "\n",
    "    This is vectorised over the RANSAC iterations but loops over points\n",
    "    because each has a different neighbourhood. Uses NumPy random sampling\n",
    "    for speed.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "    N, k = neighbor_indices.shape\n",
    "    inlier_ratios = np.zeros(N, dtype=np.float64)\n",
    "\n",
    "    neighbors = coords[neighbor_indices]  # (N, k, 3)\n",
    "\n",
    "    for i in range(N):\n",
    "        pts = neighbors[i]  # (k, 3)\n",
    "        best_inliers = 0\n",
    "\n",
    "        for _ in range(n_iterations):\n",
    "            # Sample 3 random points to define a plane\n",
    "            idx3 = rng.choice(k, size=3, replace=False)\n",
    "            p0, p1, p2 = pts[idx3[0]], pts[idx3[1]], pts[idx3[2]]\n",
    "\n",
    "            # Plane normal via cross product\n",
    "            v1 = p1 - p0\n",
    "            v2 = p2 - p0\n",
    "            normal = np.cross(v1, v2)\n",
    "            norm_len = np.linalg.norm(normal)\n",
    "            if norm_len < 1e-10:\n",
    "                continue\n",
    "            normal /= norm_len\n",
    "\n",
    "            # Distance of all k points to the plane\n",
    "            diffs = pts - p0\n",
    "            distances = np.abs(diffs @ normal)\n",
    "\n",
    "            n_inliers = np.sum(distances < distance_threshold)\n",
    "            if n_inliers > best_inliers:\n",
    "                best_inliers = n_inliers\n",
    "\n",
    "        inlier_ratios[i] = best_inliers / k\n",
    "\n",
    "    return inlier_ratios\n",
    "\n",
    "\n",
    "def _compute_multi_scale_features(\n",
    "    coords: np.ndarray,\n",
    "    tree: cKDTree,\n",
    "    scales: List[int],\n",
    "    ransac_distance: float = 0.15,\n",
    "    ransac_iterations: int = 100,\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute geometric features at multiple neighbourhood scales and aggregate.\n",
    "\n",
    "    For each scale k in `scales`, we compute the full eigenfeature set and\n",
    "    RANSAC inlier ratio, then take the **maximum** planarity / inlier ratio\n",
    "    across scales (to catch both large planar roofs and sharp edges) and the\n",
    "    **minimum** normal_z_std (tightest normal consistency at any scale).\n",
    "\n",
    "    This multi-scale approach is the key to handling:\n",
    "    - Large flat roofs (captured at k=40)\n",
    "    - Roof edges / ridgelines (captured at k=10)\n",
    "    - Pitched roofs (good RANSAC fit even when planarity is moderate)\n",
    "    \"\"\"\n",
    "    N = len(coords)\n",
    "\n",
    "    # Accumulators — we'll stack across scales and then reduce\n",
    "    all_planarity = []\n",
    "    all_linearity = []\n",
    "    all_sphericity = []\n",
    "    all_surface_var = []\n",
    "    all_normal_z = []\n",
    "    all_normal_z_std = []\n",
    "    all_ransac = []\n",
    "\n",
    "    for k_scale in scales:\n",
    "        log.info(\"  Computing features at scale k=%d...\", k_scale)\n",
    "        _, ind = tree.query(coords, k=k_scale)\n",
    "\n",
    "        feats = _compute_eigenfeatures_batch(coords, ind)\n",
    "        ransac_ratio = _ransac_plane_inlier_ratio_batch(\n",
    "            coords, ind,\n",
    "            distance_threshold=ransac_distance,\n",
    "            n_iterations=ransac_iterations,\n",
    "        )\n",
    "\n",
    "        all_planarity.append(feats['planarity'])\n",
    "        all_linearity.append(feats['linearity'])\n",
    "        all_sphericity.append(feats['sphericity'])\n",
    "        all_surface_var.append(feats['surface_variation'])\n",
    "        all_normal_z.append(feats['normal_z'])\n",
    "        all_normal_z_std.append(feats['normal_z_std'])\n",
    "        all_ransac.append(ransac_ratio)\n",
    "\n",
    "    # Stack: (n_scales, N)\n",
    "    planarity_stack = np.stack(all_planarity)\n",
    "    linearity_stack = np.stack(all_linearity)\n",
    "    sphericity_stack = np.stack(all_sphericity)\n",
    "    sv_stack = np.stack(all_surface_var)\n",
    "    nz_stack = np.stack(all_normal_z)\n",
    "    nz_std_stack = np.stack(all_normal_z_std)\n",
    "    ransac_stack = np.stack(all_ransac)\n",
    "\n",
    "    return {\n",
    "        # Take best (most building-like) value across scales\n",
    "        'planarity_max': np.max(planarity_stack, axis=0),\n",
    "        'linearity_min': np.min(linearity_stack, axis=0),\n",
    "        'sphericity_min': np.min(sphericity_stack, axis=0),\n",
    "        'surface_var_min': np.min(sv_stack, axis=0),\n",
    "        'normal_z_max': np.max(nz_stack, axis=0),\n",
    "        'normal_z_std_min': np.min(nz_std_stack, axis=0),\n",
    "        'ransac_inlier_max': np.max(ransac_stack, axis=0),\n",
    "\n",
    "        # Also keep per-scale for diagnostics\n",
    "        '_planarity_per_scale': planarity_stack,\n",
    "        '_ransac_per_scale': ransac_stack,\n",
    "    }\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# BUILDING SCORING FUNCTION (NEW)\n",
    "# =====================================================================\n",
    "def _compute_building_score(features: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Combine multiple geometric features into a single building confidence\n",
    "    score in [0, 1] using a weighted sum.\n",
    "\n",
    "    Each feature is mapped to [0, 1] with a sigmoid-like soft threshold,\n",
    "    then weighted by its discriminative importance. Weights are derived\n",
    "    from empirical analysis of Nordic ALS data but are reasonable defaults\n",
    "    for most urban/suburban scenes.\n",
    "\n",
    "    Feature contributions:\n",
    "        planarity_max     (weight 0.25) — core geometric cue\n",
    "        ransac_inlier_max (weight 0.30) — most robust single feature\n",
    "        normal_z_std_min  (weight 0.20) — penalises vegetation strongly\n",
    "        sphericity_min    (weight 0.10) — penalises scattered volumes\n",
    "        surface_var_min   (weight 0.15) — penalises rough surfaces\n",
    "\n",
    "    Returns (N,) float array of building confidence scores.\n",
    "    \"\"\"\n",
    "    def _sigmoid(x, center, steepness):\n",
    "        \"\"\"Soft threshold: maps x to [0, 1] centered at `center`.\"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-steepness * (x - center)))\n",
    "\n",
    "    # Planarity: high is good. Center at 0.4, steep transition.\n",
    "    s_plan = _sigmoid(features['planarity_max'], center=0.4, steepness=12.0)\n",
    "\n",
    "    # RANSAC inlier ratio: high is good. Center at 0.65.\n",
    "    s_ransac = _sigmoid(features['ransac_inlier_max'], center=0.65, steepness=15.0)\n",
    "\n",
    "    # Normal-Z std: LOW is good (consistent normals). Invert.\n",
    "    s_nstd = 1.0 - _sigmoid(features['normal_z_std_min'], center=0.20, steepness=20.0)\n",
    "\n",
    "    # Sphericity: LOW is good (not a ball-shaped canopy). Invert.\n",
    "    s_sph = 1.0 - _sigmoid(features['sphericity_min'], center=0.3, steepness=10.0)\n",
    "\n",
    "    # Surface variation: LOW is good. Invert.\n",
    "    s_sv = 1.0 - _sigmoid(features['surface_var_min'], center=0.15, steepness=15.0)\n",
    "\n",
    "    score = (\n",
    "        0.25 * s_plan +\n",
    "        0.30 * s_ransac +\n",
    "        0.20 * s_nstd +\n",
    "        0.10 * s_sph +\n",
    "        0.15 * s_sv\n",
    "    )\n",
    "    return score\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# REGION GROWING REFINEMENT (NEW — replaces recover_roof_edges)\n",
    "# =====================================================================\n",
    "def _region_grow_buildings(\n",
    "    coords: np.ndarray,\n",
    "    scores: np.ndarray,\n",
    "    seed_mask: np.ndarray,\n",
    "    candidate_mask: np.ndarray,\n",
    "    normals_z: np.ndarray,\n",
    "    search_radius: float = 1.5,\n",
    "    z_tolerance: float = 2.0,\n",
    "    min_score: float = 0.40,\n",
    "    normal_agreement: float = 0.85,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Region-growing from confirmed building seeds into adjacent candidates.\n",
    "\n",
    "    This replaces the separate \"recover_roof_edges\" + \"filter_building_outliers\"\n",
    "    two-step with a single principled pass that:\n",
    "      1. Starts from high-confidence building seeds.\n",
    "      2. Examines all non-seed candidates within `search_radius`.\n",
    "      3. Accepts a candidate if:\n",
    "         a. Its building score >= `min_score` (relaxed vs. seed threshold), AND\n",
    "         b. Its Z is within `z_tolerance` of the seed, AND\n",
    "         c. Its surface normal is consistent with the seed (cosine > `normal_agreement`\n",
    "            measured via the Z-component, since we're comparing flatness).\n",
    "      4. Newly accepted points become seeds for the next iteration.\n",
    "      5. Repeats until no more points are added.\n",
    "\n",
    "    This naturally captures roof edges (which have moderate scores but are\n",
    "    spatially and normally consistent with the roof) while rejecting isolated\n",
    "    vegetation (which fails the spatial + normal consistency checks).\n",
    "\n",
    "    Returns updated boolean mask of all building points.\n",
    "    \"\"\"\n",
    "    building_mask = seed_mask.copy()\n",
    "    remaining = candidate_mask & ~seed_mask\n",
    "\n",
    "    tree = cKDTree(coords)\n",
    "\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        current_building_idx = np.where(building_mask)[0]\n",
    "        current_remaining_idx = np.where(remaining)[0]\n",
    "\n",
    "        if len(current_remaining_idx) == 0:\n",
    "            break\n",
    "\n",
    "        # For each remaining candidate, find nearest building point\n",
    "        remaining_coords = coords[current_remaining_idx]\n",
    "        building_coords = coords[current_building_idx]\n",
    "\n",
    "        if len(building_coords) == 0:\n",
    "            break\n",
    "\n",
    "        btree = cKDTree(building_coords)\n",
    "        dists, nearest_bldg_local = btree.query(remaining_coords, k=1)\n",
    "\n",
    "        # Apply acceptance criteria — fully vectorised\n",
    "        nearest_bldg_global = current_building_idx[nearest_bldg_local]\n",
    "\n",
    "        cond_dist = dists <= search_radius\n",
    "        cond_z = np.abs(coords[current_remaining_idx, 2] - coords[nearest_bldg_global, 2]) <= z_tolerance\n",
    "        cond_score = scores[current_remaining_idx] >= min_score\n",
    "        cond_normal = np.abs(normals_z[current_remaining_idx] - normals_z[nearest_bldg_global]) <= (1.0 - normal_agreement)\n",
    "\n",
    "        accept = cond_dist & cond_z & cond_score & cond_normal\n",
    "        accepted_global = current_remaining_idx[accept]\n",
    "\n",
    "        if len(accepted_global) == 0:\n",
    "            break\n",
    "\n",
    "        building_mask[accepted_global] = True\n",
    "        remaining[accepted_global] = False\n",
    "        log.info(\"  Region grow iter %d: added %d points\", iteration, len(accepted_global))\n",
    "\n",
    "    return building_mask\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# MAIN RECLASSIFICATION FUNCTION (NEW — replaces 3 old functions)\n",
    "# =====================================================================\n",
    "def reclassify_buildings(\n",
    "    input_path: str,\n",
    "    output_path: str,\n",
    "    cfg: PipelineConfig,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Multi-feature, multi-scale building reclassification with region growing.\n",
    "\n",
    "    Replaces the old three-function chain:\n",
    "        reclassify_buildings_from_veg → filter_building_outliers → recover_roof_edges\n",
    "\n",
    "    HAG is computed in-memory and used ONLY for candidate filtering.\n",
    "    The output file retains the original elevation Z values.\n",
    "\n",
    "    Pipeline:\n",
    "        1. Compute HAG in-memory (original Z untouched)\n",
    "        2. HAG filter — exclude ground and extreme outliers\n",
    "        3. Multi-scale eigenfeatures + RANSAC on candidates (using original elevation)\n",
    "        4. Weighted score computation\n",
    "        5. High-confidence seeds (score > threshold)\n",
    "        6. Region-growing to capture edges\n",
    "        7. Cluster-based cleanup of remaining noise\n",
    "        8. Write output with original elevation preserved\n",
    "    \"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "    las = laspy.read(input_path)\n",
    "    log.info(\"Loaded %s points for building reclassification.\", f\"{len(las.points):,}\")\n",
    "\n",
    "    # Original elevation coordinates — used for ALL geometry and preserved in output\n",
    "    coords = np.vstack((las.x, las.y, las.z)).T\n",
    "    classification = np.asarray(las.classification).copy()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # STEP 1: Compute HAG in-memory (Z is NOT overwritten)\n",
    "    # ------------------------------------------------------------------\n",
    "    hag = _compute_hag(las, k=cfg.hag_k_neighbors)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # STEP 2: HAG-based candidate selection\n",
    "    # ------------------------------------------------------------------\n",
    "    # Use the in-memory HAG array to filter candidates by height,\n",
    "    # but all subsequent geometry uses the original elevation coords.\n",
    "    candidate_mask = (\n",
    "        np.isin(classification, [1, 5]) &   # Unclassified or High Vegetation\n",
    "        (hag >= cfg.reclass_min_hag) &\n",
    "        (hag <= cfg.reclass_max_hag)\n",
    "    )\n",
    "\n",
    "    candidate_indices = np.where(candidate_mask)[0]\n",
    "    log.info(\"Step 2 — HAG filter: %s candidates (%.1f–%.1f m above ground)\",\n",
    "             f\"{len(candidate_indices):,}\", cfg.reclass_min_hag, cfg.reclass_max_hag)\n",
    "\n",
    "    if len(candidate_indices) == 0:\n",
    "        log.warning(\"No candidates after HAG filter. Check min_hag / max_hag settings.\")\n",
    "        las.write(output_path)\n",
    "        return\n",
    "\n",
    "    # Use original-elevation coords for all geometric analysis\n",
    "    cand_coords = coords[candidate_indices]\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # STEP 3: Multi-scale geometric features\n",
    "    # ------------------------------------------------------------------\n",
    "    log.info(\"Step 3 — Computing multi-scale features at k=%s...\", cfg.reclass_scales)\n",
    "    tree = cKDTree(cand_coords)\n",
    "    ms_features = _compute_multi_scale_features(\n",
    "        cand_coords, tree,\n",
    "        scales=cfg.reclass_scales,\n",
    "        ransac_distance=cfg.reclass_ransac_distance,\n",
    "        ransac_iterations=cfg.reclass_ransac_iterations,\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # STEP 4: Building score\n",
    "    # ------------------------------------------------------------------\n",
    "    log.info(\"Step 4 — Computing building confidence scores...\")\n",
    "    scores = _compute_building_score(ms_features)\n",
    "\n",
    "    log.info(\"  Score distribution: min=%.3f  median=%.3f  mean=%.3f  max=%.3f\",\n",
    "             np.min(scores), np.median(scores), np.mean(scores), np.max(scores))\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # STEP 5: High-confidence seeds\n",
    "    # ------------------------------------------------------------------\n",
    "    seed_local_mask = scores >= cfg.reclass_score_threshold\n",
    "    n_seeds = int(np.sum(seed_local_mask))\n",
    "    log.info(\"Step 4 — Seeds (score >= %.2f): %s points\",\n",
    "             cfg.reclass_score_threshold, f\"{n_seeds:,}\")\n",
    "\n",
    "    if n_seeds == 0:\n",
    "        log.warning(\"No seeds found. Consider lowering reclass_score_threshold.\")\n",
    "        las.write(output_path)\n",
    "        return\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # STEP 6: Region growing\n",
    "    # ------------------------------------------------------------------\n",
    "    log.info(\"Step 6 — Region growing (radius=%.1f m, z_tol=%.1f m, min_score=%.2f)...\",\n",
    "             cfg.rg_search_radius, cfg.rg_z_tolerance, cfg.rg_min_score)\n",
    "\n",
    "    # We need normal_z for the agreement check\n",
    "    # Use the largest scale for the most stable normals\n",
    "    max_k = max(cfg.reclass_scales)\n",
    "    _, ind_norms = tree.query(cand_coords, k=max_k)\n",
    "    norm_feats = _compute_eigenfeatures_batch(cand_coords, ind_norms)\n",
    "    normals_z = norm_feats['normal_z']\n",
    "\n",
    "    all_candidate_local = np.ones(len(cand_coords), dtype=bool)\n",
    "    building_local_mask = _region_grow_buildings(\n",
    "        coords=cand_coords,\n",
    "        scores=scores,\n",
    "        seed_mask=seed_local_mask,\n",
    "        candidate_mask=all_candidate_local,\n",
    "        normals_z=normals_z,\n",
    "        search_radius=cfg.rg_search_radius,\n",
    "        z_tolerance=cfg.rg_z_tolerance,\n",
    "        min_score=cfg.rg_min_score,\n",
    "        normal_agreement=cfg.rg_normal_agreement,\n",
    "    )\n",
    "\n",
    "    n_after_rg = int(np.sum(building_local_mask))\n",
    "    log.info(\"  After region growing: %s building points (grew %s from seeds)\",\n",
    "             f\"{n_after_rg:,}\", f\"{n_after_rg - n_seeds:,}\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # STEP 7: Cluster cleanup — remove tiny isolated clusters\n",
    "    # ------------------------------------------------------------------\n",
    "    log.info(\"Step 7 — Cluster-based cleanup (min_size=%d)...\", cfg.cluster_min_size)\n",
    "    bldg_local_idx = np.where(building_local_mask)[0]\n",
    "\n",
    "    if len(bldg_local_idx) > 0:\n",
    "        bldg_coords = cand_coords[bldg_local_idx]\n",
    "\n",
    "        if cfg.use_hdbscan and HAS_HDBSCAN:\n",
    "            clusterer = HDBSCAN(min_cluster_size=cfg.cluster_min_size, min_samples=10)\n",
    "            labels = clusterer.fit_predict(bldg_coords)\n",
    "        else:\n",
    "            labels = DBSCAN(eps=cfg.cluster_eps, min_samples=10).fit_predict(bldg_coords)\n",
    "\n",
    "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "        size_map = dict(zip(unique_labels, counts))\n",
    "        point_sizes = np.array([size_map[l] for l in labels])\n",
    "\n",
    "        valid = (labels != -1) & (point_sizes >= cfg.cluster_min_size)\n",
    "        building_local_mask[bldg_local_idx[~valid]] = False\n",
    "\n",
    "        n_reverted = int(np.sum(~valid))\n",
    "        n_final = int(np.sum(building_local_mask))\n",
    "        n_valid_clusters = int(np.sum((unique_labels != -1) & (counts >= cfg.cluster_min_size)))\n",
    "        log.info(\"  Cleanup: reverted %d spurious points, kept %d in %d clusters.\",\n",
    "                 n_reverted, n_final, n_valid_clusters)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # APPLY to LAS classification\n",
    "    # ------------------------------------------------------------------\n",
    "    building_global_idx = candidate_indices[building_local_mask]\n",
    "    classification[building_global_idx] = 6\n",
    "    las.classification = classification\n",
    "\n",
    "    las.write(output_path)\n",
    "    log.info(\"Building reclassification complete in %.1f s. Saved to %s\",\n",
    "             time.perf_counter() - t0, output_path)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# VISUALIZATION (expanded)\n",
    "# =====================================================================\n",
    "def visualize_classification(las_file_path: str) -> None:\n",
    "    las = laspy.read(las_file_path)\n",
    "    points = np.vstack((las.x, las.y, las.z)).T\n",
    "    classification = np.asarray(las.classification)\n",
    "\n",
    "    color_map = {\n",
    "        2: [0.6, 0.4, 0.2],   # Ground\n",
    "        3: [0.0, 0.9, 0.0],   # Low Veg\n",
    "        4: [0.0, 0.7, 0.0],   # Med Veg\n",
    "        5: [0.0, 0.5, 0.0],   # High Veg\n",
    "        6: [1.0, 0.0, 0.0],   # Building\n",
    "    }\n",
    "    colors = np.tile([0.5, 0.5, 0.5], (len(points), 1))\n",
    "    for cls, rgb in color_map.items():\n",
    "        colors[classification == cls] = rgb\n",
    "\n",
    "    for cls in sorted(color_map):\n",
    "        c = int(np.sum(classification == cls))\n",
    "        if c > 0:\n",
    "            log.info(\"Class %d: %s points\", cls, f\"{c:,}\")\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window(window_name=\"LiDAR Classification Viewer\")\n",
    "    vis.add_geometry(pcd)\n",
    "    opt = vis.get_render_option()\n",
    "    opt.background_color = np.asarray([0, 0, 0])\n",
    "    vis.run()\n",
    "    vis.destroy_window()\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# PIPELINE\n",
    "# =====================================================================\n",
    "def run_pipeline(cfg: PipelineConfig) -> None:\n",
    "    \"\"\"\n",
    "    Upgraded pipeline order:\n",
    "        1. Crop\n",
    "        2. SOR + class filter\n",
    "        3. Building reclassification (computes HAG internally, outputs original elevation)\n",
    "        4. Visualise\n",
    "    \"\"\"\n",
    "    input_laz = \"data/study_area.laz\"\n",
    "    cropped = \"data/cropped.laz\"\n",
    "    filtered = \"output/filtered.laz\"\n",
    "    reclassified = \"output/reclassified_final.laz\"\n",
    "\n",
    "    os.makedirs(\"output\", exist_ok=True)\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    crop_las_file(input_laz, cropped, bounds=cfg.bounds)\n",
    "    filter_lidar_data(cropped, filtered,\n",
    "                      nb_neighbors=cfg.sor_nb_neighbors,\n",
    "                      std_ratio=cfg.sor_std_ratio,\n",
    "                      valid_classes=cfg.valid_classes,\n",
    "                      chunk_size=cfg.sor_chunk_size)\n",
    "\n",
    "    # Reclassification computes HAG in-memory for candidate filtering,\n",
    "    # but the output file retains the original elevation Z values.\n",
    "    reclassify_buildings(filtered, reclassified, cfg=cfg)\n",
    "\n",
    "    log.info(\"Full pipeline finished in %.1f s.\", time.perf_counter() - t0)\n",
    "    visualize_classification(reclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-21 00:51:47,363 [INFO] Loaded 15,413,412 points from data/study_area.laz\n",
      "2026-02-21 00:51:47,476 [INFO] Saving 2,328,282 cropped points.\n",
      "2026-02-21 00:51:47,645 [INFO] Running chunked SOR on 2,328,282 points (k=20, σ=3.0)...\n",
      "2026-02-21 00:51:49,535 [INFO] SOR removed 30,851 outliers.\n",
      "2026-02-21 00:51:49,609 [INFO] After class filter: 2,297,097 points.\n",
      "2026-02-21 00:51:49,765 [INFO] Loaded 2,297,097 points for building reclassification.\n",
      "2026-02-21 00:51:49,801 [INFO] Computing HAG from 793,227 ground points (IDW k=4)...\n",
      "2026-02-21 00:51:51,370 [INFO] HAG range: 0.00 – 33.15 m\n",
      "2026-02-21 00:51:51,381 [INFO] Step 2 — HAG filter: 1,251,288 candidates (2.0–50.0 m above ground)\n",
      "2026-02-21 00:51:51,388 [INFO] Step 3 — Computing multi-scale features at k=[10, 20, 40]...\n",
      "2026-02-21 00:51:51,583 [INFO]   Computing features at scale k=10...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m\n\u001b[1;32m      5\u001b[0m target_x, target_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m532885\u001b[39m, \u001b[38;5;241m6983510\u001b[39m\n\u001b[1;32m      7\u001b[0m config \u001b[38;5;241m=\u001b[39m PipelineConfig(\n\u001b[1;32m      8\u001b[0m     bounds\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_x\u001b[39m\u001b[38;5;124m'\u001b[39m: target_x \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m150\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     use_hdbscan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     35\u001b[0m )\n\u001b[0;32m---> 37\u001b[0m \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 774\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    766\u001b[0m filter_lidar_data(cropped, filtered,\n\u001b[1;32m    767\u001b[0m                   nb_neighbors\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39msor_nb_neighbors,\n\u001b[1;32m    768\u001b[0m                   std_ratio\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39msor_std_ratio,\n\u001b[1;32m    769\u001b[0m                   valid_classes\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mvalid_classes,\n\u001b[1;32m    770\u001b[0m                   chunk_size\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39msor_chunk_size)\n\u001b[1;32m    772\u001b[0m \u001b[38;5;66;03m# Reclassification computes HAG in-memory for candidate filtering,\u001b[39;00m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;66;03m# but the output file retains the original elevation Z values.\u001b[39;00m\n\u001b[0;32m--> 774\u001b[0m \u001b[43mreclassify_buildings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreclassified\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    776\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull pipeline finished in \u001b[39m\u001b[38;5;132;01m%.1f\u001b[39;00m\u001b[38;5;124m s.\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m t0)\n\u001b[1;32m    777\u001b[0m visualize_classification(reclassified)\n",
      "Cell \u001b[0;32mIn[4], line 610\u001b[0m, in \u001b[0;36mreclassify_buildings\u001b[0;34m(input_path, output_path, cfg)\u001b[0m\n\u001b[1;32m    608\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep 3 — Computing multi-scale features at k=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m, cfg\u001b[38;5;241m.\u001b[39mreclass_scales)\n\u001b[1;32m    609\u001b[0m tree \u001b[38;5;241m=\u001b[39m cKDTree(cand_coords)\n\u001b[0;32m--> 610\u001b[0m ms_features \u001b[38;5;241m=\u001b[39m \u001b[43m_compute_multi_scale_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcand_coords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscales\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreclass_scales\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mransac_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreclass_ransac_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mransac_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreclass_ransac_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;66;03m# STEP 4: Building score\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    620\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep 4 — Computing building confidence scores...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 372\u001b[0m, in \u001b[0;36m_compute_multi_scale_features\u001b[0;34m(coords, tree, scales, ransac_distance, ransac_iterations)\u001b[0m\n\u001b[1;32m    369\u001b[0m _, ind \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mquery(coords, k\u001b[38;5;241m=\u001b[39mk_scale)\n\u001b[1;32m    371\u001b[0m feats \u001b[38;5;241m=\u001b[39m _compute_eigenfeatures_batch(coords, ind)\n\u001b[0;32m--> 372\u001b[0m ransac_ratio \u001b[38;5;241m=\u001b[39m \u001b[43m_ransac_plane_inlier_ratio_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistance_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mransac_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mransac_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m all_planarity\u001b[38;5;241m.\u001b[39mappend(feats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplanarity\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    379\u001b[0m all_linearity\u001b[38;5;241m.\u001b[39mappend(feats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinearity\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[4], line 311\u001b[0m, in \u001b[0;36m_ransac_plane_inlier_ratio_batch\u001b[0;34m(coords, neighbor_indices, distance_threshold, n_iterations, rng_seed)\u001b[0m\n\u001b[1;32m    307\u001b[0m best_inliers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iterations):\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# Sample 3 random points to define a plane\u001b[39;00m\n\u001b[0;32m--> 311\u001b[0m     idx3 \u001b[38;5;241m=\u001b[39m \u001b[43mrng\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m     p0, p1, p2 \u001b[38;5;241m=\u001b[39m pts[idx3[\u001b[38;5;241m0\u001b[39m]], pts[idx3[\u001b[38;5;241m1\u001b[39m]], pts[idx3[\u001b[38;5;241m2\u001b[39m]]\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;66;03m# Plane normal via cross product\u001b[39;00m\n",
      "File \u001b[0;32mnumpy/random/_generator.pyx:967\u001b[0m, in \u001b[0;36mnumpy.random._generator.Generator.choice\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sb310/lib/python3.10/site-packages/numpy/_core/numeric.py:300\u001b[0m, in \u001b[0;36mfull\u001b[0;34m(shape, fill_value, dtype, order, device, like)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_full_dispatcher\u001b[39m(\n\u001b[1;32m    295\u001b[0m     shape, fill_value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, like\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    296\u001b[0m ):\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(like,)\n\u001b[0;32m--> 300\u001b[0m \u001b[38;5;129m@set_array_function_like_doc\u001b[39m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfull\u001b[39m(shape, fill_value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, like\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    303\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03m    Return a new array of given shape and type, filled with `fill_value`.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m like \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# MAIN\n",
    "# =====================================================================\n",
    "\n",
    "target_x, target_y = 532885, 6983510\n",
    "\n",
    "config = PipelineConfig(\n",
    "    bounds={\n",
    "        'min_x': target_x - 150,\n",
    "        'max_x': target_x + 350,\n",
    "        'min_y': target_y - 350,\n",
    "        'max_y': target_y + 150,\n",
    "    },\n",
    "    # SOR\n",
    "    sor_nb_neighbors=20,\n",
    "    sor_std_ratio=3.0,\n",
    "    # HAG\n",
    "    hag_k_neighbors=4,\n",
    "    # Building reclassification\n",
    "    reclass_scales=[10, 20, 40],\n",
    "    reclass_min_hag=2.0,\n",
    "    reclass_max_hag=50.0,\n",
    "    reclass_score_threshold=0.55,\n",
    "    reclass_ransac_distance=0.15,\n",
    "    reclass_ransac_iterations=100,\n",
    "    reclass_normal_std_threshold=0.25,\n",
    "    # Region growing\n",
    "    rg_search_radius=1.5,\n",
    "    rg_z_tolerance=2.0,\n",
    "    rg_min_score=0.40,\n",
    "    rg_normal_agreement=0.85,\n",
    "    # Cluster cleanup\n",
    "    cluster_min_size=100,\n",
    "    use_hdbscan=True,\n",
    ")\n",
    "\n",
    "run_pipeline(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
