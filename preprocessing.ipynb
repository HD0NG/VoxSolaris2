{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Pre Processing\n",
    "1. Merge\n",
    "2. Crop\n",
    "3. Statistical Outlier Removal (Noise Filtering)\n",
    "4. Filter classes\n",
    "5. Reclassify\n",
    "6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import laspy\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KDTree\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.cluster import DBSCAN\n",
    "import open3d as o3d\n",
    "import rasterio\n",
    "from pyproj import CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_las_files(path1, path2, output_path):\n",
    "    print(f\"Reading {path1} and {path2}...\")\n",
    "    las1 = laspy.read(path1)\n",
    "    las2 = laspy.read(path2)\n",
    "\n",
    "    # 1. Ensure point formats match (e.g., both are Format 6 or Format 1)\n",
    "    if las1.header.point_format.id != las2.header.point_format.id:\n",
    "        raise ValueError(\"Point formats do not match! Cannot merge different formats easily.\")\n",
    "\n",
    "    # 2. Setup the new header\n",
    "    # We use the scales and offsets from the first file as our global reference\n",
    "    new_header = laspy.LasHeader(\n",
    "        point_format=las1.header.point_format, \n",
    "        version=las1.header.version\n",
    "    )\n",
    "    new_header.offsets = las1.header.offsets\n",
    "    new_header.scales = las1.header.scales\n",
    "\n",
    "    # 3. CONCATENATE THE UNDERLYING ARRAYS\n",
    "    # This avoids the ScaleAwarePointRecord TypeError\n",
    "    merged_array = np.concatenate([las1.points.array, las2.points.array])\n",
    "\n",
    "    # 4. Create the new LAS object and assign the array\n",
    "    merged_las = laspy.LasData(new_header)\n",
    "    merged_las.points = laspy.ScaleAwarePointRecord(\n",
    "        merged_array, \n",
    "        las1.header.point_format, \n",
    "        las1.header.scales, \n",
    "        las1.header.offsets\n",
    "    )\n",
    "\n",
    "    # 5. Write to disk\n",
    "    print(f\"Writing {len(merged_array)} points to {output_path}...\")\n",
    "    merged_las.write(output_path)\n",
    "    print(\"Merge successful.\")\n",
    "\n",
    "# --- Usage ---\n",
    "# merge_las_files(\"data/tile_A.laz\", \"data/tile_B.laz\", \"data/merged_study_area.laz\")\n",
    "\n",
    "def crop_las_file(input_path, output_path, bounds):\n",
    "    \"\"\"\n",
    "    Crops a LAS/LAZ file based on specific coordinate bounds.\n",
    "    :param bounds: A dictionary with {'min_x', 'max_x', 'min_y', 'max_y'}\n",
    "    \"\"\"\n",
    "    print(f\"Loading {input_path}...\")\n",
    "    las = laspy.read(input_path)\n",
    "    \n",
    "    # 1. Extract Bounds from the dictionary\n",
    "    min_x, max_x = bounds['min_x'], bounds['max_x']\n",
    "    min_y, max_y = bounds['min_y'], bounds['max_y']\n",
    "    \n",
    "    print(f\"Cropping to custom Bounds:\")\n",
    "    print(f\" - X: {min_x} to {max_x}\")\n",
    "    print(f\" - Y: {min_y} to {max_y}\")\n",
    "\n",
    "    # 2. Create Boolean Mask\n",
    "    mask = (\n",
    "        (las.x >= min_x) & (las.x <= max_x) & \n",
    "        (las.y >= min_y) & (las.y <= max_y)\n",
    "    )\n",
    "    \n",
    "    # 3. Apply Mask\n",
    "    cropped_points = las.points[mask]\n",
    "    \n",
    "    if len(cropped_points) == 0:\n",
    "        print(\"Error: No points found in the specified range!\")\n",
    "        return\n",
    "\n",
    "    # 4. Create New LAS Data\n",
    "    new_header = laspy.LasHeader(point_format=las.header.point_format, version=las.header.version)\n",
    "    new_header.offsets = las.header.offsets\n",
    "    new_header.scales = las.header.scales\n",
    "    \n",
    "    cropped_las = laspy.LasData(new_header)\n",
    "    cropped_las.points = cropped_points\n",
    "\n",
    "    print(f\"Saving {len(cropped_points)} points to {output_path}...\")\n",
    "    cropped_las.write(output_path)\n",
    "    print(\"Done.\")\n",
    "\n",
    "# --- Usage for your \"Study House\" Site ---\n",
    "# Let's say your house is at (532934, 6983793) \n",
    "# and you want 50m to the West and 150m to the East (Asymmetrical)\n",
    "\n",
    "# target_x = 532934.33\n",
    "# target_y = 6983793.63\n",
    "\n",
    "# custom_bounds = {\n",
    "#     'min_x': target_x - 50,   # 50m left\n",
    "#     'max_x': target_x + 150,  # 150m right\n",
    "#     'min_y': target_y - 100,  # 100m down\n",
    "#     'max_y': target_y + 100   # 100m up\n",
    "# }\n",
    "\n",
    "# crop_las_file(\"input.laz\", \"study_house_area.laz\", custom_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lidar_data(input_path, output_path, nb_neighbors=20, std_ratio=2.0):\n",
    "    \"\"\"\n",
    "    Processes raw LiDAR: Reads LAS -> Statistical Outlier Removal -> Classification Filter.\n",
    "    \n",
    "    Args:\n",
    "        input_path (str): Path to the raw .las or .laz file.\n",
    "        output_path (str): Path to save the processed file.\n",
    "        nb_neighbors (int): K-neighbors for SOR filter.\n",
    "        std_ratio (float): Standard deviation multiplier for SOR filter.\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting Processing for {os.path.basename(input_path)} ---\")\n",
    "    \n",
    "    # 1. Load LAS file\n",
    "    las = laspy.read(input_path)\n",
    "    points = np.vstack((las.x, las.y, las.z)).transpose()\n",
    "    \n",
    "    # 2. Statistical Outlier Removal (SOR) via Open3D\n",
    "    print(\"Running Statistical Outlier Removal...\")\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    \n",
    "    # cl: cleaned cloud (unused here), ind: indices of points to keep\n",
    "    _, ind = pcd.remove_statistical_outlier(nb_neighbors=nb_neighbors, std_ratio=std_ratio)\n",
    "    \n",
    "    # Apply the SOR indices to the original las object to keep all dimensions (intensity, etc.)\n",
    "    las = las[ind]\n",
    "    print(f\"SOR complete. Removed {len(points) - len(las)} outliers.\")\n",
    "\n",
    "    # 3. Filter by Classification (Classes 2, 3, 4, 5, 6)\n",
    "    print(\"Filtering for classes 2, 3, 4, 5, and 6...\")\n",
    "    valid_classes = [2, 3, 4, 5, 6]\n",
    "    class_mask = np.isin(las.classification, valid_classes)\n",
    "    \n",
    "    final_las = las[class_mask]\n",
    "    \n",
    "    # 4. Save results\n",
    "    final_las.write(output_path)\n",
    "    print(f\"Successfully saved {len(final_las)} points to {output_path}\")\n",
    "    print(\"--- Processing Complete ---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reclassify_buildings_from_veg(input_path, output_path, neighbor_k=20, planarity_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Reclassifies points labeled as High Vegetation (5) to Buildings (6) \n",
    "    based on geometric planarity.\n",
    "    \"\"\"\n",
    "    print(f\"Loading {input_path}...\")\n",
    "    las = laspy.read(input_path)\n",
    "    \n",
    "    # 1. Identify candidate points\n",
    "    # Assuming 'High Vegetation' is standard ASPRS Class 5\n",
    "    # We want to potentially move these to Building (Class 6)\n",
    "    candidate_mask = las.classification == 5\n",
    "    \n",
    "    # If no high vegetation exists, check if they are unclassified (1) or created roughly\n",
    "    if np.sum(candidate_mask) == 0:\n",
    "        print(\"Warning: No points found in Class 5 (High Vegetation). Checking Class 1 (Unclassified)...\")\n",
    "        candidate_mask = las.classification == 1\n",
    "        \n",
    "    candidate_indices = np.where(candidate_mask)[0]\n",
    "    print(f\"Analyzing {len(candidate_indices)} candidate points...\")\n",
    "\n",
    "    if len(candidate_indices) == 0:\n",
    "        print(\"No points to process.\")\n",
    "        return\n",
    "\n",
    "    # 2. Prepare coordinates for KDTree\n",
    "    # Stack X, Y, Z into a (N, 3) array\n",
    "    coords = np.vstack((las.x, las.y, las.z)).transpose()\n",
    "    candidate_coords = coords[candidate_indices]\n",
    "\n",
    "    # 3. Build KDTree for geometric analysis\n",
    "    # This allows us to find the nearest neighbors for every point efficiently\n",
    "    print(\"Building KDTree and computing geometric features...\")\n",
    "    tree = KDTree(candidate_coords, leaf_size=40)\n",
    "    \n",
    "    # Query k-nearest neighbors (k=20 is usually sufficient for structure detection)\n",
    "    dist, ind = tree.query(candidate_coords, k=neighbor_k)\n",
    "    \n",
    "    # 4. Eigenvalue computation loop\n",
    "    # We need to compute the covariance matrix for each point's neighborhood\n",
    "    # and extract eigenvalues to determine if the neighborhood is flat (planar) or scattered.\n",
    "    \n",
    "    new_building_indices = []\n",
    "    \n",
    "    for i, point_neighbors in enumerate(ind):\n",
    "        # Get coordinates of the neighbors\n",
    "        neighbor_xyz = candidate_coords[point_neighbors]\n",
    "        \n",
    "        # Compute Covariance Matrix\n",
    "        cov = np.cov(neighbor_xyz, rowvar=False)\n",
    "        \n",
    "        # Get Eigenvalues and sort them (e1 >= e2 >= e3)\n",
    "        eigenvalues = np.linalg.eigvalsh(cov)\n",
    "        # eigvalsh returns them in ascending order, so:\n",
    "        e3, e2, e1 = eigenvalues[0], eigenvalues[1], eigenvalues[2]\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if e1 == 0:\n",
    "            continue\n",
    "            \n",
    "        # 5. Calculate Planarity\n",
    "        # Planarity = (e2 - e3) / e1\n",
    "        # High planarity (closer to 1) indicates a flat surface (Building)\n",
    "        # Low planarity (closer to 0) indicates a rough/scattered volume (Tree)\n",
    "        planarity = (e2 - e3) / e1\n",
    "        \n",
    "        if planarity > planarity_threshold:\n",
    "            # Map back to the original LAS index\n",
    "            original_idx = candidate_indices[i]\n",
    "            new_building_indices.append(original_idx)\n",
    "\n",
    "    # 6. Apply Reclassification\n",
    "    # ASPRS Standard: Building = 6\n",
    "    print(f\"Reclassifying {len(new_building_indices)} points as Buildings (Class 6)...\")\n",
    "    \n",
    "    # We modify the classification array in place\n",
    "    if len(new_building_indices) > 0:\n",
    "        las.classification[new_building_indices] = 6\n",
    "\n",
    "    # 7. Save output\n",
    "    print(f\"Saving result to {output_path}...\")\n",
    "    las.write(output_path)\n",
    "    print(\"Done!\")\n",
    "\n",
    "def recover_roof_edges(input_path, output_path, search_radius=1.5, z_tolerance=0.5, loose_threshold=0.35):\n",
    "    \"\"\"\n",
    "    Expands the building classification to include roof edges that were missed\n",
    "    due to lower planarity scores.\n",
    "    \n",
    "    Parameters:\n",
    "    - search_radius: Max distance to look for a 'core' building point.\n",
    "    - z_tolerance: Max vertical difference allowed (prevents grabbing ground/trees below).\n",
    "    - loose_threshold: The relaxed planarity score for edges (usually 0.3 to 0.5).\n",
    "    \"\"\"\n",
    "    print(f\"Loading {input_path} for Edge Recovery...\")\n",
    "    las = laspy.read(input_path)\n",
    "    \n",
    "    # 1. Separate \"Core Buildings\" and \"Candidates\"\n",
    "    # Core = Already classified as 6\n",
    "    # Candidates = Vegetation (5) or Unclassified (1)\n",
    "    core_mask = las.classification == 6\n",
    "    cand_mask = np.isin(las.classification, [1, 5])\n",
    "    \n",
    "    core_indices = np.where(core_mask)[0]\n",
    "    cand_indices = np.where(cand_mask)[0]\n",
    "    \n",
    "    if len(core_indices) == 0 or len(cand_indices) == 0:\n",
    "        print(\"Not enough points to process.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Refining edges using {len(core_indices)} core building points...\")\n",
    "\n",
    "    # 2. Re-calculate Planarity for Candidates (Crucial Step)\n",
    "    # We need to know which candidates are 'flat-ish' (edges) vs 'scattered' (trees)\n",
    "    coords = np.vstack((las.x, las.y, las.z)).transpose()\n",
    "    cand_coords = coords[cand_indices]\n",
    "    \n",
    "    # Build tree on candidates to compute THEIR planarity\n",
    "    # (Using k=15, slightly smaller context than before to catch sharper edges)\n",
    "    tree_geom = KDTree(cand_coords, leaf_size=40)\n",
    "    dist, ind = tree_geom.query(cand_coords, k=15)\n",
    "    \n",
    "    # Identify \"Weak Candidates\" (Planarity between loose_threshold and 0.6)\n",
    "    weak_candidate_local_indices = []\n",
    "    \n",
    "    for i, point_neighbors in enumerate(ind):\n",
    "        neighbor_xyz = cand_coords[point_neighbors]\n",
    "        cov = np.cov(neighbor_xyz, rowvar=False)\n",
    "        eigenvalues = np.linalg.eigvalsh(cov)\n",
    "        e3, e2, e1 = eigenvalues\n",
    "        \n",
    "        if e1 == 0: continue\n",
    "            \n",
    "        planarity = (e2 - e3) / e1\n",
    "        \n",
    "        # We accept lower planarity here because we will add a proximity check later\n",
    "        if planarity > loose_threshold:\n",
    "            weak_candidate_local_indices.append(i)\n",
    "\n",
    "    # Convert local list to original LAS indices\n",
    "    # These are points that LOOK like edges but weren't strict enough to be buildings\n",
    "    potential_edge_indices = cand_indices[weak_candidate_local_indices]\n",
    "    potential_edge_coords = coords[potential_edge_indices]\n",
    "\n",
    "    print(f\"Found {len(potential_edge_indices)} potential edge points (Planarity > {loose_threshold}).\")\n",
    "    print(\"Verifying spatial connection to core buildings...\")\n",
    "\n",
    "    # 3. Spatial Verification (The \"Anchor\" Check)\n",
    "    # Build a tree of the CONFIRMED buildings\n",
    "    core_coords = coords[core_indices]\n",
    "    core_tree = KDTree(core_coords, leaf_size=40)\n",
    "    \n",
    "    # For every potential edge point, find the CLOSEST core building point\n",
    "    dists, nearest_core_indices = core_tree.query(potential_edge_coords, k=1)\n",
    "    \n",
    "    points_to_upgrade = []\n",
    "    \n",
    "    for i, (d, core_idx_rel) in enumerate(zip(dists, nearest_core_indices)):\n",
    "        d = d[0] # Distance to nearest building\n",
    "        \n",
    "        # Condition A: Must be horizontally close (within 1.5m)\n",
    "        if d > search_radius:\n",
    "            continue\n",
    "            \n",
    "        # Condition B: Must be vertically aligned (prevent grabbing the ground below an eave)\n",
    "        # Get Z of the edge point and Z of the nearest building point\n",
    "        z_edge = potential_edge_coords[i][2]\n",
    "        z_core = core_coords[core_idx_rel[0]][2]\n",
    "        \n",
    "        if abs(z_edge - z_core) < z_tolerance:\n",
    "            points_to_upgrade.append(potential_edge_indices[i])\n",
    "            \n",
    "    # 4. Apply Changes\n",
    "    if points_to_upgrade:\n",
    "        las.classification[points_to_upgrade] = 6\n",
    "        print(f\"Recovered {len(points_to_upgrade)} edge points!\")\n",
    "    else:\n",
    "        print(\"No edges recovered.\")\n",
    "        \n",
    "    las.write(output_path)\n",
    "    print(\"Done.\")\n",
    "\n",
    "def filter_building_outliers(input_path, output_path, eps=1.5, min_cluster_size=100):\n",
    "    \"\"\"\n",
    "    Removes small, isolated clusters of points classified as Building (6)\n",
    "    and reverts them to High Vegetation (5).\n",
    "    \n",
    "    Parameters:\n",
    "    - eps: The maximum distance between two points to be considered neighbors (in meters).\n",
    "           Increase this if your building points have gaps (e.g., lower density).\n",
    "    - min_cluster_size: Minimum number of points required to constitute a 'real' building.\n",
    "    \"\"\"\n",
    "    print(f\"Loading {input_path}...\")\n",
    "    las = laspy.read(input_path)\n",
    "    \n",
    "    # 1. Select only current Building points\n",
    "    building_mask = las.classification == 6\n",
    "    building_indices = np.where(building_mask)[0]\n",
    "    \n",
    "    if len(building_indices) == 0:\n",
    "        print(\"No building points found to filter.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Clustering {len(building_indices)} building points...\")\n",
    "    \n",
    "    # Get coordinates of building points\n",
    "    # We use only XY for clustering if we want to treat a building as a single footprint,\n",
    "    # but using XYZ is safer to avoid merging a low tree with a high roof.\n",
    "    building_coords = np.vstack((las.x[building_indices], \n",
    "                                 las.y[building_indices], \n",
    "                                 las.z[building_indices])).transpose()\n",
    "\n",
    "    # 2. Run DBSCAN Clustering\n",
    "    # eps=1.5m means points within 1.5m of each other are part of the same object\n",
    "    # min_samples=10 ensures we don't start a cluster on pure noise\n",
    "    db = DBSCAN(eps=eps, min_samples=10).fit(building_coords)\n",
    "    labels = db.labels_\n",
    "\n",
    "    # 3. Analyze Cluster Sizes\n",
    "    # labels == -1 are noise (points that didn't even form a small cluster)\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    \n",
    "    # Identify which labels are \"valid buildings\" (large enough)\n",
    "    valid_labels = set()\n",
    "    \n",
    "    # Track stats\n",
    "    noise_points = 0\n",
    "    small_cluster_points = 0\n",
    "    \n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        if label == -1:\n",
    "            noise_points += count\n",
    "            continue\n",
    "            \n",
    "        if count >= min_cluster_size:\n",
    "            valid_labels.add(label)\n",
    "        else:\n",
    "            small_cluster_points += count\n",
    "\n",
    "    print(f\"Filtering Report:\")\n",
    "    print(f\" - Total Clusters Found: {len(unique_labels)}\")\n",
    "    print(f\" - Noise Points Removed: {noise_points}\")\n",
    "    print(f\" - Small Cluster Points Reverted: {small_cluster_points}\")\n",
    "    print(f\" - Real Buildings Kept: {len(valid_labels)}\")\n",
    "\n",
    "    # 4. Apply Filtering\n",
    "    # We iterate through our local building_indices. \n",
    "    # If their cluster label is NOT in valid_labels, we revert them.\n",
    "    \n",
    "    points_to_revert = []\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        if label not in valid_labels:\n",
    "            # Get the original index in the LAS file\n",
    "            original_idx = building_indices[i]\n",
    "            points_to_revert.append(original_idx)\n",
    "            \n",
    "    # 5. Reclassify\n",
    "    if points_to_revert:\n",
    "        # Revert to High Vegetation (5)\n",
    "        las.classification[points_to_revert] = 5\n",
    "        \n",
    "    print(f\"Saving cleaned file to {output_path}...\")\n",
    "    las.write(output_path)\n",
    "    print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_hag(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Normalizes the LAS file Z coordinates to Height Above Ground (HAG).\n",
    "    \"\"\"\n",
    "    print(f\"Loading {input_path} for HAG conversion...\")\n",
    "    las = laspy.read(input_path)\n",
    "    \n",
    "    # 1. Extract Ground Points (Class 2)\n",
    "    ground_mask = las.classification == 2\n",
    "    \n",
    "    # Fallback: If no class 2, assume the lowest 1% of points are ground\n",
    "    if np.sum(ground_mask) == 0:\n",
    "        print(\"Warning: No Ground Class (2) found. Estimating ground from lowest points...\")\n",
    "        # Sort by Z and take the bottom 1% as a proxy for ground\n",
    "        sorted_indices = np.argsort(las.z)\n",
    "        n_ground = int(len(las.points) * 0.01)\n",
    "        ground_indices = sorted_indices[:n_ground]\n",
    "        ground_mask = np.zeros(len(las.points), dtype=bool)\n",
    "        ground_mask[ground_indices] = True\n",
    "    \n",
    "    ground_points = np.vstack((las.x[ground_mask], \n",
    "                               las.y[ground_mask], \n",
    "                               las.z[ground_mask])).transpose()\n",
    "    \n",
    "    print(f\"Using {len(ground_points)} ground points to build DTM...\")\n",
    "\n",
    "    # 2. Build 2D KDTree for Ground (X, Y only)\n",
    "    # This allows us to find the nearest ground point for every other point\n",
    "    ground_xy = ground_points[:, :2]\n",
    "    ground_z = ground_points[:, 2]\n",
    "    \n",
    "    tree = cKDTree(ground_xy)\n",
    "    \n",
    "    # 3. Query Nearest Ground Point for ALL points\n",
    "    # We query the X,Y of all points against the ground tree\n",
    "    all_xy = np.vstack((las.x, las.y)).transpose()\n",
    "    \n",
    "    # k=1 means find the single nearest ground point (fastest method)\n",
    "    # For smoother terrain, you could use k=3 and average them, but k=1 is sufficient for 100m.\n",
    "    dists, indices = tree.query(all_xy, k=1)\n",
    "    \n",
    "    # 4. Calculate HAG\n",
    "    # Z_ground_ref is the Z value of the nearest ground neighbor\n",
    "    z_ground_ref = ground_z[indices]\n",
    "    \n",
    "    hag_values = las.z - z_ground_ref\n",
    "    \n",
    "    # 5. Update Z values\n",
    "    # We overwrite the Z dimension. Now Z=0 means \"On the ground\".\n",
    "    las.z = hag_values\n",
    "    \n",
    "    print(f\"HAG Calculated. Min Z: {np.min(las.z):.2f}, Max Z: {np.max(las.z):.2f}\")\n",
    "    \n",
    "    # Optional: Reset ground points exactly to 0 to remove noise\n",
    "    las.z[ground_mask] = 0.0\n",
    "    \n",
    "    print(f\"Saving HAG file to {output_path}...\")\n",
    "    las.write(output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_classification(las_file_path):\n",
    "    print(f\"Loading {las_file_path} for visualization...\")\n",
    "    las = laspy.read(las_file_path)\n",
    "    \n",
    "    # 1. Extract coordinates\n",
    "    # We stack them into an (N, 3) array that Open3D expects\n",
    "    points = np.vstack((las.x, las.y, las.z)).transpose()\n",
    "    \n",
    "    # 2. Extract classification\n",
    "    classification = las.classification\n",
    "    \n",
    "    # 3. Create a Color Map\n",
    "    # Initialize all points to Gray (default for unclassified/other)\n",
    "    # Shape must be (N, 3) for RGB channels, values 0.0 to 1.0\n",
    "    colors = np.zeros((len(points), 3))\n",
    "    colors[:] = [0.5, 0.5, 0.5]  # Grey\n",
    "    \n",
    "    # Color Class 5 (High Vegetation) -> Green\n",
    "    veg_indices = np.where(classification == 5)[0]\n",
    "    colors[veg_indices] = [0.0, 0.6, 0.0]  # Dark Green\n",
    "    \n",
    "    # Color Class 6 (Building) -> Red\n",
    "    bldg_indices = np.where(classification == 6)[0]\n",
    "    colors[bldg_indices] = [1.0, 0.0, 0.0]  # Bright Red\n",
    "    \n",
    "    print(f\"Stats:\")\n",
    "    print(f\" - Vegetation (Green): {len(veg_indices)} points\")\n",
    "    print(f\" - Buildings (Red):   {len(bldg_indices)} points\")\n",
    "    \n",
    "    # 4. Create Open3D PointCloud Object\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    \n",
    "    # 5. Visualize\n",
    "    print(\"Opening visualizer... (Use mouse to rotate, scroll to zoom)\")\n",
    "    \n",
    "    # We create a visualization window with a black background for better contrast\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window(window_name=\"Building vs Vegetation Check\")\n",
    "    vis.add_geometry(pcd)\n",
    "    \n",
    "    # Set background to black\n",
    "    opt = vis.get_render_option()\n",
    "    opt.background_color = np.asarray([0, 0, 0])\n",
    "    \n",
    "    vis.run()\n",
    "    vis.destroy_window()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_laz_merged = \"data/study_area.laz\"\n",
    "\n",
    "cropped_laz = \"data/P5123C2_9_cropped.laz\"\n",
    "filtered_laz = \"output/filtered.laz\"\n",
    "# hag_laz = \"data/P5123C2_9_clipped_SE_hag.laz\"\n",
    "reclassified_laz = \"output/reclassified.laz\"\n",
    "recovered_laz = \"output/recovered.laz\"\n",
    "cleaned_laz = \"output/cleaned.laz\"\n",
    "\n",
    "target_x, target_y = 532885, 6983510\n",
    "\n",
    "custom_bounds = {\n",
    "    'min_x': target_x - 150,   # 150m left\n",
    "    'max_x': target_x + 350,  # 350m right\n",
    "    'min_y': target_y - 350,  # 350m down\n",
    "    'max_y': target_y + 150   # 150m up\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/study_area.laz...\n",
      "Cropping to custom Bounds:\n",
      " - X: 532735 to 533235\n",
      " - Y: 6983160 to 6983660\n",
      "Saving 2328282 points to data/P5123C2_9_cropped.laz...\n",
      "Done.\n",
      "--- Starting Processing for P5123C2_9_cropped.laz ---\n",
      "Running Statistical Outlier Removal...\n",
      "SOR complete. Removed 30851 outliers.\n",
      "Filtering for classes 2, 3, 4, 5, and 6...\n",
      "Successfully saved 2297097 points to output/filtered.laz\n",
      "--- Processing Complete ---\n",
      "\n",
      "Loading output/filtered.laz...\n",
      "Analyzing 1252631 candidate points...\n",
      "Building KDTree and computing geometric features...\n",
      "Reclassifying 173890 points as Buildings (Class 6)...\n",
      "Saving result to output/reclassified.laz...\n",
      "Done!\n",
      "Loading output/reclassified.laz...\n",
      "Clustering 173890 building points...\n",
      "Filtering Report:\n",
      " - Total Clusters Found: 2662\n",
      " - Noise Points Removed: 115729\n",
      " - Small Cluster Points Reverted: 50034\n",
      " - Real Buildings Kept: 20\n",
      "Saving cleaned file to output/cleaned.laz...\n",
      "Done.\n",
      "Loading output/cleaned.laz for Edge Recovery...\n",
      "Refining edges using 8127 core building points...\n",
      "Found 750029 potential edge points (Planarity > 0.25).\n",
      "Verifying spatial connection to core buildings...\n",
      "Recovered 2032 edge points!\n",
      "Done.\n",
      "Loading output/recovered.laz for visualization...\n",
      "Stats:\n",
      " - Vegetation (Green): 1242472 points\n",
      " - Buildings (Red):   10159 points\n",
      "Opening visualizer... (Use mouse to rotate, scroll to zoom)\n"
     ]
    }
   ],
   "source": [
    "crop_las_file(input_laz_merged, cropped_laz, custom_bounds)\n",
    "filter_lidar_data(cropped_laz, filtered_laz, nb_neighbors=20, std_ratio=3.0)\n",
    "reclassify_buildings_from_veg(filtered_laz, reclassified_laz, planarity_threshold=0.5)\n",
    "filter_building_outliers(reclassified_laz, cleaned_laz, eps=1.5, min_cluster_size=120)\n",
    "recover_roof_edges(cleaned_laz, recovered_laz, loose_threshold=0.25, search_radius=2.5, z_tolerance=2.5)\n",
    "visualize_classification(recovered_laz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "csv_path = os.path.join(\"output\", \"study_area_polygon.csv\")\n",
    "# 2. Write the WKT (Well-Known Text) Polygon to CSV\n",
    "# This format allows QGIS to recognize the shape immediately\n",
    "wkt_polygon = (f\"POLYGON (({custom_bounds['min_x']} {custom_bounds['min_y']}, \"\n",
    "               f\"{custom_bounds['max_x']} {custom_bounds['min_y']}, \"\n",
    "               f\"{custom_bounds['max_x']} {custom_bounds['max_y']}, \"\n",
    "               f\"{custom_bounds['min_x']} {custom_bounds['max_y']}, \"\n",
    "               f\"{custom_bounds['min_x']} {custom_bounds['min_y']}))\")\n",
    "\n",
    "with open(csv_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"name\", \"wkt\"])\n",
    "    writer.writerow([1, \"Solar_Study_Area\", wkt_polygon])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
